
1. Component Architecture
- **Major Components and Interactions:**
  - **Data Ingestion Component:** Reads data from `agilisium_playground.purgo_playground.drug_inventory_management` table.
  - **Data Quality Check Component:** Implements data quality rules from `DQ_rules_IM` sheet using PySpark/Spark SQL.
  - **Notification Component:** Sends alerts if any data quality rule is violated.
  - **Data Quality Result Component:** Creates a dataframe summarizing the results of the data quality checks.

- **Input/Output Interfaces:**
  - **Input:** 
    - Source: `agilisium_playground.purgo_playground.drug_inventory_management` table.
    - Rules: `DQ_rules_IM` sheet from `data_quality_mapping.xlsx`.
  - **Output:**
    - Data Quality Dataframe with columns: `check_name`, `result`, `pass_%`.
    - Notification alerts for rule violations.

- **Dependencies and External Systems:**
  - **Databricks Environment:** For executing PySpark/Spark SQL code.
  - **Excel Reader Library:** To read `data_quality_mapping.xlsx`.
  - **Notification System:** Email or logging system for alerts.

2. Data Flow
- **Data Transformation Steps:**
  1. Load data from the `drug_inventory_management` table.
  2. Apply each data quality rule from `DQ_rules_IM`:
     - **Mandatory Fields Check:** Verify non-null values for specified fields.
     - **Expiry Date Check:** Ensure `expiry_date` > `purchase_date`.
     - **Unique Check:** Validate uniqueness of `Product ID` and `Batch number`.
     - **Data Consistency Check:** Ensure `quantity` > 0 and date formats are correct.

- **Data Formats and Schemas:**
  - **Input Schema:** Matches the structure of `drug_inventory_management` table.
  - **Output Schema:** Data Quality Dataframe with:
    
    check_name: STRING
    result: STRING (e.g., "Pass", "Fail")
    pass_%: DOUBLE
    

- **Validation Rules and Error Handling:**
  - **Mandatory Fields Check:** Raise an error if any field is null.
  - **Expiry Date Check:** Log an error if `expiry_date` <= `purchase_date`.
  - **Unique Check:** Log duplicates if found.
  - **Data Consistency Check:** Log errors for negative quantities or incorrect date formats.

3. Implementation Steps
- **Step 1: Setup Environment**
  - Configure Databricks cluster and import necessary libraries.
  - Acceptance Criteria: Environment is ready for data processing.

- **Step 2: Load Data**
  - Load data from `drug_inventory_management` table.
  - Acceptance Criteria: Data is successfully loaded into a DataFrame.

- **Step 3: Implement Data Quality Checks**
  - Write PySpark/Spark SQL code for each rule in `DQ_rules_IM`.
  - Acceptance Criteria: Each rule is implemented and tested individually.

- **Step 4: Generate Data Quality Dataframe**
  - Aggregate results into a DataFrame with `check_name`, `result`, `pass_%`.
  - Acceptance Criteria: Data Quality Dataframe is created with accurate results.

- **Step 5: Implement Notification System**
  - Configure and test notification alerts for rule violations.
  - Acceptance Criteria: Notifications are sent upon rule violations.

4. Technical Considerations
- **Performance Requirements:**
  - Optimize Spark jobs for efficient data processing.
  - Ensure minimal latency in data quality checks.

- **Security Considerations:**
  - Secure access to the `drug_inventory_management` table.
  - Ensure data privacy and compliance with relevant regulations.

- **Scalability Aspects:**
  - Design the system to handle increasing data volumes.
  - Ensure the architecture supports additional data quality rules in the future.