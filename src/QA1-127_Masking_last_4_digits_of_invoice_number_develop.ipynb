{"cells": [{"cell_type": "code", "metadata": {}, "source": ["%py\n# PySpark code to mask the last 4 digits of invoice numbers in d_product_revenue_clone table\n\nfrom pyspark.sql.functions import when, lit, length, substring, col\n\ntry:\n    # Load data from d_product_revenue (replace with your actual table path if it's in a file)\n    df = spark.table(\"d_product_revenue\")  # Assuming it's a table\nexcept Exception as e:\n    print(f\"Error loading data: {e}\")\n    # Handle the error appropriately (e.g., exit, log, use a default DataFrame)\n    # Example: create an empty DataFrame with the correct schema if the table doesn't exist\n    from pyspark.sql.types import StructType, StructField, StringType, LongType  # Import required types\n    schema = StructType([\n        StructField(\"invoice_number\", StringType(), True),\n        # Add other columns from d_product_revenue schema here\n    ])\n    df = spark.createDataFrame([], schema=schema)\n\n# Create or replace the clone table (drop if it exists)\nspark.sql(\"DROP TABLE IF EXISTS d_product_revenue_clone\")\ndf.write.saveAsTable(\"d_product_revenue_clone\")\n\n# Load data from the clone table (important to reload after creating the clone)\ndf_clone = spark.table(\"d_product_revenue_clone\")\n\n# Improved Masking Logic (handles NULLs, empty strings, and short invoice numbers)\ndf_masked = df_clone.withColumn(\"masked_invoice\", when(\n    col(\"invoice_number\").isNull() | (col(\"invoice_number\") == \"\"),  # Handle NULLs and empty strings\n    col(\"invoice_number\")  # Keep them as NULLs or empty strings\n).otherwise(\n    when(\n        length(col(\"invoice_number\")) <= 4,\n        lit(\"****\")  # Mask entirely if <= 4 chars\n    ).otherwise(\n        substring(col(\"invoice_number\"), 1, length(col(\"invoice_number\")) - 4) + lit(\"****\")  # Mask last 4\n    )\n))\n\n\n\n# Overwrite the invoice_number column in d_product_revenue_clone with the masked values\ndf_masked = df_masked.drop(\"invoice_number\").withColumnRenamed(\"masked_invoice\",\"invoice_number\") # Drop original and rename\n\ndf_masked.write.mode(\"overwrite\").saveAsTable(\"d_product_revenue_clone\")\n\n\n# Perform Validations against the modified table\n# ... (validation code from the provided test cases can be added here)\n\n\n# Example data type validation after overwriting the column:\nresult_schema = spark.table(\"d_product_revenue_clone\").schema\ninvoice_data_type = result_schema[\"invoice_number\"].dataType\nassert str(invoice_data_type) == \"StringType\", f\"Data type validation failed. Expected StringType, but got {invoice_data_type}\"\n\n# Add more validation tests as needed\n\n#Final comment indicating successful execution\n\n"], "outputs": [], "execution_count": null}], "metadata": {}, "nbformat": 4, "nbformat_minor": 4}